{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bad16a",
   "metadata": {},
   "source": [
    "# Data loading \n",
    "\n",
    "Here we will be using the ```.paraquet``` file we downloaded and do the following:\n",
    " - Check metadata and table datatypes of the paraquet file/table\n",
    " - Convert the paraquet file to pandas dataframe and check the datatypes. Additionally check the data dictionary to make sure you have the right datatypes in pandas, as pandas will automatically create the table in our database.\n",
    " - Generate the DDL CREATE statement from pandas for a sanity check.\n",
    " - Create a connection to our database using SQLAlchemy\n",
    " - Convert our huge paraquet file into a iterable that has batches of 100,000 rows and load it into our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afef2456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:55:14.141738Z",
     "start_time": "2023-12-03T23:55:14.124217Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pyarrow.parquet as pq\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a863a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"yellow_tripdata_2024-01.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c750d1d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T02:54:01.925350Z",
     "start_time": "2023-12-03T02:54:01.661119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x0000024B79E11A30>\n",
       "  created_by: parquet-cpp-arrow version 14.0.2\n",
       "  num_columns: 19\n",
       "  num_rows: 2964624\n",
       "  num_row_groups: 3\n",
       "  format_version: 2.6\n",
       "  serialized_size: 6357"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read metadata \n",
    "pq.read_metadata(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158ce65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2964624 entries, 0 to 2964623\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int32         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int32         \n",
      " 8   DOLocationID           int32         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  Airport_fee            float64       \n",
      "dtypes: datetime64[us](2), float64(12), int32(3), int64(1), object(1)\n",
      "memory usage: 395.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(file_name)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf039a0",
   "metadata": {},
   "source": [
    "We need to first create the connection to our postgres database. We can feed the connection information to generate the CREATE SQL query for the specific server. SQLAlchemy supports a variety of servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e701ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T22:50:25.811951Z",
     "start_time": "2023-12-03T22:50:25.393987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x24b7cc7e630>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an open SQL database connection object or a SQLAlchemy connectable\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96a1075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T22:50:43.628727Z",
     "start_time": "2023-12-03T22:50:43.442337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" INTEGER, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" INTEGER, \n",
      "\t\"DOLocationID\" INTEGER, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53), \n",
      "\t\"Airport_fee\" FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate CREATE SQL statement from schema for validation\n",
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7f32d",
   "metadata": {},
   "source": [
    "Datatypes for the table looks good! Since we used paraquet file the datasets seem to have been preserved. You may have to convert some datatypes so it is always good to do this check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a751ed",
   "metadata": {},
   "source": [
    "## Finally inserting data\n",
    "\n",
    "There are 2,846,722 rows in our dataset. We are going to use the ```parquet_file.iter_batches()``` function to create batches of 100,000, convert them into pandas and then load it into the postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e20cec73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:49:28.768786Z",
     "start_time": "2023-12-03T23:49:28.689732Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This part is for testing\n",
    "\n",
    "\n",
    "# Creating batches of 100,000 for the paraquet file\n",
    "file = pq.ParquetFile(file_name)\n",
    "batches_iter = file.iter_batches(batch_size=100000)\n",
    "batches_iter\n",
    "\n",
    "# Take the first batch for testing\n",
    "df = next(batches_iter).to_pandas()\n",
    "df\n",
    "\n",
    "# Creating just the table in postgres\n",
    "# df.head(0).to_sql(name='ny_taxi_data',con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513f7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_sql(name='ny_taxi_data',con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fdda025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T00:08:07.651559Z",
     "start_time": "2023-12-04T00:02:35.940526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting batch 1...\n",
      "inserted! time taken     16.433 seconds.\n",
      "\n",
      "inserting batch 2...\n",
      "inserted! time taken     13.760 seconds.\n",
      "\n",
      "inserting batch 3...\n",
      "inserted! time taken     18.953 seconds.\n",
      "\n",
      "inserting batch 4...\n",
      "inserted! time taken     55.153 seconds.\n",
      "\n",
      "inserting batch 5...\n",
      "inserted! time taken     46.887 seconds.\n",
      "\n",
      "inserting batch 6...\n",
      "inserted! time taken     34.767 seconds.\n",
      "\n",
      "inserting batch 7...\n",
      "inserted! time taken     32.688 seconds.\n",
      "\n",
      "inserting batch 8...\n",
      "inserted! time taken     24.360 seconds.\n",
      "\n",
      "inserting batch 9...\n",
      "inserted! time taken     30.315 seconds.\n",
      "\n",
      "inserting batch 10...\n",
      "inserted! time taken     35.865 seconds.\n",
      "\n",
      "inserting batch 11...\n",
      "inserted! time taken     33.675 seconds.\n",
      "\n",
      "inserting batch 12...\n",
      "inserted! time taken     33.297 seconds.\n",
      "\n",
      "inserting batch 13...\n",
      "inserted! time taken     20.051 seconds.\n",
      "\n",
      "inserting batch 14...\n",
      "inserted! time taken     14.541 seconds.\n",
      "\n",
      "inserting batch 15...\n",
      "inserted! time taken     33.542 seconds.\n",
      "\n",
      "inserting batch 16...\n",
      "inserted! time taken     27.633 seconds.\n",
      "\n",
      "inserting batch 17...\n",
      "inserted! time taken     11.712 seconds.\n",
      "\n",
      "inserting batch 18...\n",
      "inserted! time taken     14.399 seconds.\n",
      "\n",
      "inserting batch 19...\n",
      "inserted! time taken     22.105 seconds.\n",
      "\n",
      "inserting batch 20...\n",
      "inserted! time taken     13.548 seconds.\n",
      "\n",
      "inserting batch 21...\n",
      "inserted! time taken     18.396 seconds.\n",
      "\n",
      "inserting batch 22...\n",
      "inserted! time taken     19.286 seconds.\n",
      "\n",
      "inserting batch 23...\n",
      "inserted! time taken     17.203 seconds.\n",
      "\n",
      "inserting batch 24...\n",
      "inserted! time taken     15.337 seconds.\n",
      "\n",
      "inserting batch 25...\n",
      "inserted! time taken     38.639 seconds.\n",
      "\n",
      "inserting batch 26...\n",
      "inserted! time taken     17.560 seconds.\n",
      "\n",
      "inserting batch 27...\n",
      "inserted! time taken     12.029 seconds.\n",
      "\n",
      "inserting batch 28...\n",
      "inserted! time taken     10.998 seconds.\n",
      "\n",
      "inserting batch 29...\n",
      "inserted! time taken     10.796 seconds.\n",
      "\n",
      "inserting batch 30...\n",
      "inserted! time taken      6.229 seconds.\n",
      "\n",
      "Completed! Total time taken was    700.757 seconds for 30 batches.\n"
     ]
    }
   ],
   "source": [
    "# Insert values into the table \n",
    "t_start = time()\n",
    "count = 0\n",
    "file = pq.ParquetFile(file_name)\n",
    "for batch in file.iter_batches(batch_size=100000):\n",
    "    count+=1\n",
    "    batch_df = batch.to_pandas()\n",
    "    print(f'inserting batch {count}...')\n",
    "    b_start = time()\n",
    "    \n",
    "    batch_df.to_sql(name='ny_taxi_data',con=engine, if_exists='append')\n",
    "    b_end = time()\n",
    "    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n",
    "    \n",
    "t_end = time()   \n",
    "print(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c102be",
   "metadata": {},
   "source": [
    "## Extra bit\n",
    "\n",
    "While trying to do the SQL Refresher, there was a need to add a lookup zones table but the file is in ```.csv``` format. \n",
    "\n",
    "Let's code to handle both ```.csv``` and ```.paraquet``` files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643d171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:59:29.236458Z",
     "start_time": "2023-12-05T20:59:28.551221Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd \n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9040a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:18:11.346552Z",
     "start_time": "2023-12-05T21:18:11.337475Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv'\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet'\n",
    "\n",
    "file_name = url.rsplit('/', 1)[-1].strip()\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495fa96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:18:33.001561Z",
     "start_time": "2023-12-05T21:18:32.844872Z"
    }
   },
   "outputs": [],
   "source": [
    "if '.csv' in file_name:\n",
    "    print('yay') \n",
    "    df = pd.read_csv(file_name, nrows=10)\n",
    "    df_iter = pd.read_csv(file_name, iterator=True, chunksize=100000)\n",
    "elif '.parquet' in file_name:\n",
    "    print('oh yea')\n",
    "    file = pq.ParquetFile(file_name)\n",
    "    df = next(file.iter_batches(batch_size=10)).to_pandas()\n",
    "    df_iter = file.iter_batches(batch_size=100000)\n",
    "else: \n",
    "    print('Error. Only .csv or .parquet files allowed.')\n",
    "    sys.exit() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556748f",
   "metadata": {},
   "source": [
    "This code is a rough code and seems to be working. The cleaned up version will be in `data-loading-parquet.py` file."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
